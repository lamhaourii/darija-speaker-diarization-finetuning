{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93bc89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbe8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LUFS = -23.0\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./downloaded_wav\"\n",
    "\n",
    "subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "\n",
    "for folder in subfolders:\n",
    "    \n",
    "    files = [f for f in os.listdir(os.path.join(path, folder)) if os.path.isfile(os.path.join(os.path.join(path, folder), f))]\n",
    "    dir=os.path.join(path,folder,'normalized')\n",
    "    os.makedirs(dir,exist_ok=True)\n",
    "    for file in files:\n",
    "        audio, sr = librosa.load(os.path.join(path,folder,file), mono=True,sr=SAMPLE_RATE)\n",
    "        meter = pyln.Meter(sr)\n",
    "        loudness = meter.integrated_loudness(audio)\n",
    "        normalized_audio = pyln.normalize.loudness(audio, loudness, TARGET_LUFS)\n",
    "        peak = np.max(np.abs(normalized_audio))\n",
    "        if peak > 1.0:\n",
    "            normalized_audio = normalized_audio / peak * 0.99\n",
    "        save_path=os.path.join(dir,f\"{file}_normalized.wav\")\n",
    "        sf.write(save_path, normalized_audio, SAMPLE_RATE)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d552cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchaudio\\_internal\\module_utils.py:71: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b501c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ŸÅÿ±ÿßÿ¨Ÿä ÔΩú Faraji',\n",
       " 'Amina Bamo',\n",
       " 'Blogbiba',\n",
       " 'Ghita Mizdid',\n",
       " 'Hayat Bensaleh#',\n",
       " 'Ichrak vibes',\n",
       " 'Ilyan Zar ÔΩú ÿ±ÿ¨ÿßŸÑ ÿßŸÑŸÜÿÆÿ®ÿ©',\n",
       " 'Kawtar Bamo',\n",
       " 'Life in DE by Malak üéÄ',\n",
       " 'mahamawda',\n",
       " 'mahamawda extra',\n",
       " 'milfaya',\n",
       " 'montakhab fans',\n",
       " 'Mustapha Swinga',\n",
       " 'Othmane SAFSAFI',\n",
       " 'Rachid Achachi - ÿ±ÿ¥ŸäÿØ ÿπÿ¥ÿπÿßÿ¥Ÿä',\n",
       " 'Rajaa bd',\n",
       " 'wissal aamir',\n",
       " 'With Nassima',\n",
       " 'zoz vlogs',\n",
       " 'ÿ£ŸÖŸäŸÜ ÿßŸÑÿπŸàŸÜŸä ‚ß∏ Amine Aouni']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = \"./downloaded_wav\"\n",
    "\n",
    "subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ea05794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ŸÅÿ±ÿßÿ¨Ÿä ÔΩú Faraji',\n",
       " 'Amina Bamo',\n",
       " 'Blogbiba',\n",
       " 'Ghita Mizdid',\n",
       " 'Hayat Bensaleh#',\n",
       " 'Ichrak vibes',\n",
       " 'Ilyan Zar ÔΩú ÿ±ÿ¨ÿßŸÑ ÿßŸÑŸÜÿÆÿ®ÿ©',\n",
       " 'Kawtar Bamo',\n",
       " 'Life in DE by Malak üéÄ',\n",
       " 'mahamawda',\n",
       " 'mahamawda extra',\n",
       " 'milfaya',\n",
       " 'montakhab fans',\n",
       " 'Mustapha Swinga',\n",
       " 'Othmane SAFSAFI',\n",
       " 'Rachid Achachi - ÿ±ÿ¥ŸäÿØ ÿπÿ¥ÿπÿßÿ¥Ÿä',\n",
       " 'Rajaa bd',\n",
       " 'wissal aamir',\n",
       " 'With Nassima',\n",
       " 'zoz vlogs',\n",
       " 'ÿ£ŸÖŸäŸÜ ÿßŸÑÿπŸàŸÜŸä ‚ß∏ Amine Aouni']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd19bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = [f for f in os.listdir(folder) if os.path.isdir(os.path.join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d5234fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normalized']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder2=os.path.join(path,folder)\n",
    "os.listdir(folder2)\n",
    "folder3=[f for f in os.listdir(folder2) if os.path.isdir(os.path.join(folder2, f))]\n",
    "folder3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94536cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e66b15a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import pyannote.audio\n",
    "print(pyannote.audio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(os.path.join(path, folder)) if os.path.isfile(os.path.join(os.path.join(path, folder), f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "daeb18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEN = 3.5               \n",
    "MAX_LEN = 4.5\n",
    "SR = 16000                  \n",
    "INPUT_DIR=\"./downloaded_wav\"\n",
    "OUTPUT_DIR = \"cliips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdcb71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_wav_random_clips(input_path, output_speaker_dir, clip_id_start=0):\n",
    "    y, sr = librosa.load(input_path, sr=SR, mono=True)\n",
    "    duration = len(y) / sr\n",
    "    t = 0.0\n",
    "    clip_id = clip_id_start\n",
    "\n",
    "    while t < duration:\n",
    "        clip_len = random.uniform(MIN_LEN, MAX_LEN)\n",
    "        end = min(t + clip_len, duration)\n",
    "        clip_dur = end - t\n",
    "\n",
    "        if clip_dur < MIN_LEN:\n",
    "            break  \n",
    "\n",
    "        start_sample = int(t * sr)\n",
    "        end_sample = int(end * sr)\n",
    "        clip = y[start_sample:end_sample]\n",
    "\n",
    "        os.makedirs(output_speaker_dir, exist_ok=True)\n",
    "        out_path = os.path.join(output_speaker_dir, f\"clip_{clip_id:04d}.wav\")\n",
    "        sf.write(out_path, clip, sr)\n",
    "\n",
    "        clip_id += 1\n",
    "        t = end\n",
    "    return clip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "299f6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_id = 0\n",
    "for speaker_folder in os.listdir(INPUT_DIR):\n",
    "    speaker_path = os.path.join(INPUT_DIR, speaker_folder)\n",
    "    path=os.path.join(speaker_path,'normalized')\n",
    "\n",
    "    output_speaker_dir = os.path.join(OUTPUT_DIR, speaker_folder)\n",
    "\n",
    "    for wav_file in os.listdir(path):\n",
    "        if wav_file.endswith(\".wav\"):\n",
    "            input_path = os.path.join(path, wav_file)\n",
    "            clip_id = split_wav_random_clips(input_path, output_speaker_dir, clip_id)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be47ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ŸÅÿ±ÿßÿ¨Ÿä ÔΩú Faraji',\n",
       " 'Amina Bamo',\n",
       " 'Blogbiba',\n",
       " 'Ghita Mizdid',\n",
       " 'Hayat Bensaleh#',\n",
       " 'Ichrak vibes',\n",
       " 'Ilyan Zar ÔΩú ÿ±ÿ¨ÿßŸÑ ÿßŸÑŸÜÿÆÿ®ÿ©',\n",
       " 'Kawtar Bamo',\n",
       " 'mahamawda',\n",
       " 'milfaya',\n",
       " 'montakhab fans',\n",
       " 'Mustapha Swinga',\n",
       " 'Othmane SAFSAFI',\n",
       " 'Rachid Achachi - ÿ±ÿ¥ŸäÿØ ÿπÿ¥ÿπÿßÿ¥Ÿä',\n",
       " 'Rajaa bd',\n",
       " 'wissal aamir',\n",
       " 'With Nassima',\n",
       " 'zoz vlogs',\n",
       " 'ÿ£ŸÖŸäŸÜ ÿßŸÑÿπŸàŸÜŸä ‚ß∏ Amine Aouni']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l='./cliips'\n",
    "spkrs=os.listdir(l)\n",
    "spkrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28642366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "403\n",
      "399\n",
      "176\n",
      "418\n",
      "214\n",
      "263\n",
      "150\n",
      "565\n",
      "359\n",
      "367\n",
      "484\n",
      "729\n",
      "152\n",
      "437\n",
      "337\n",
      "280\n",
      "395\n",
      "389\n"
     ]
    }
   ],
   "source": [
    "for s in spkrs:\n",
    "    print(len(os.listdir(os.path.join(l,s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv created with 6851 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "DATASET_DIR = \"cliips\"\n",
    "SPLIT = \"train\"\n",
    "OUTPUT_CSV = os.path.join(DATASET_DIR, \"metadata.csv\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "split_dir = os.path.join(DATASET_DIR, SPLIT)\n",
    "\n",
    "for speaker in sorted(os.listdir(split_dir)):\n",
    "    speaker_path = os.path.join(split_dir, speaker)\n",
    "\n",
    "    if not os.path.isdir(speaker_path):\n",
    "        continue\n",
    "\n",
    "    for file in sorted(os.listdir(speaker_path)):\n",
    "        if file.lower().endswith((\".wav\", \".flac\", \".mp3\")):\n",
    "            file_path = os.path.join(SPLIT, speaker, file)\n",
    "            rows.append([file_path, speaker])\n",
    "\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"file_name\", \"label\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"metadata.csv created with {len(rows)} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b5d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouha\\Desktop\\2a\\audio_project\\darija\\synth_data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6851/6851 [00:00<00:00, 44442.96files/s]\n",
      "Generating train split: 6851 examples [00:05, 1304.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "local_dataset = load_dataset(\"audiofolder\", data_dir=\"cliips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a6b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "from dotenv import load_dotev\n",
    "load_dotev()\n",
    "hf_token=os.getenv(\"hf_token\")\n",
    "huggingface_hub.login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647f22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3426/3426 [00:04<00:00, 813.17 examples/s]hards/s]\n",
      "Creating parquet from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.19ba/s]\n",
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  429MB /  429MB, 5.73MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  429MB /  429MB, 5.73MB/s  \n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3425/3425 [00:04<00:00, 835.25 examples/s] 62.05s/ shards]\n",
      "Creating parquet from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.88ba/s]\n",
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  436MB /  436MB, 6.24MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  436MB /  436MB, 6.24MB/s  \n",
      "Uploading the dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [02:04<00:00, 62.10s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/igitsml/darija-speakers-clips/commit/d556bbbc671e659a91e9929fe670db16a29405c0', commit_message='Upload dataset', commit_description='', oid='d556bbbc671e659a91e9929fe670db16a29405c0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/igitsml/darija-speakers-clips', endpoint='https://huggingface.co', repo_type='dataset', repo_id='igitsml/darija-speakers-clips'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dataset.push_to_hub(\"igitsml/darija-speakers-clips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa92cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
